
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference]{IEEEtran}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}

\usepackage{graphicx}
\usepackage{float}


% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{From intention to code review - 
%How long does it take? 
the valuable metrics. The case study of a cloud infrastructure.}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Dorealda Dalipaj}
\IEEEauthorblockA{Universidad Rey Juan Carlos\\
LibreSoft\\
Madrid, Spain\\
dorealda.dalipaj@urjc.es}
\and
\IEEEauthorblockN{Jesus M. Gonzales Barahona}
\IEEEauthorblockA{Universidad Rey Juan Carlos\\
GSyC/LibreSoft\\
Madrid, Spain\\
jgb@gsyc.es}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
The purpose of code review is to find and fix the source code defects that can cause vulnerabilities 
and thus give rise to an exploit. The code review process encloses a collection of metrics 
which can be used to improve the software development process.
Benefits from those metrics varies from measuring the progress of a development team to 
investigating into software development policies and guidelines.
\\From this excellent source of metrics we highlight \emph{bug triaging} and \emph{time to review}.
These metrics do not implicate subjective relations. They are only material facts. As such, 
they can be recorded to trace the outcome of the code review process.
\\We have analysed those two metrics along the line of the development process 
%, from 
%the intention up to the moment the code actually runs into the code base, 
of a large 
cloud operating system, OpenStack. Finally we related our results with those of previous 
studies and draw different conclusions. 
\\We believe that the monitoring and management of both these metrics can produce direct benefit on the 
effectiveness of the code review process, by being able to identify which development practices 
provide more value than others.


\end{abstract}

\begin{IEEEkeywords}
code, review, process, metrics, software, engineering.

\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\section{Introduction}

Software development process is characterized by a sequence 
%nderthurje  
of different activities, with the intent of better planning and management. 
Some of the activities depend on technical factors, while others are merely organizational 
and administering issue dependent.
\\
In one of the phases of software development, at some point, the implementation of features starts. 
Consequently, when during the testing phase errors are found, 
developers proceed to triage the bugs. Finally, depending on the result of this activity,
a review of that chunk of code containing that bug may start. 
\\
This process ends when the code with the bug fixed runs into the code base of the project, and whoever might be 
waiting takes dependency on the new code.
\\
In this paper we take into consideration and analyse exactly the span of time that extends 
from finding the errors up to when the 
new code, with the errors fixed, runs into the code base. We will call this span the time from \textbf{I}ntention to 
\textbf{C}ode \textbf{R}eview and will refer to it as \textbf{ICR} in the remaining of the paper.    
\\
The ICR process encloses a collection of metrics 
which can be used to improve the software development process.
These software metrics can be grouped in two distinct classes: relative and absolute.
While relative metrics are a representation of an attribute which can not be directly 
measured and rely on the context where the metric was derived, 
absolute metrics are numerical values which describe a trait of the code that do not 
involve subjective context but are only material fact.
\\
With former code inspection, many of this metrics could not be measured, but today, with the 
modern systems, we can trace the information needed to analyse them. 
\\ 
The final purpose of this study is to bring into focus two absolute metrics from ICR process: 
\emph{time spent for bug triaging} (\textbf{BTT})  and \emph{time spend to review the code} (\textbf{CRT}).
\\
We are going to quantify the duration of the ICR process and measure both BTT and CRT 
metrics of the projects we selected as our case study. These measures will 
conceptually help us to proportion the size that BTT and CRT have on ICR, 
consequently the relations between the metrics and the process.
\\
Our proposal is that the two metrics, BTT and CRT, are very important for different reasons. 
First, through them we can better characterise the ICR process. 
Second and most important, if supervising these two metrics, we believe that 
the code review process can significantly improve.
\\
With the abundance of data and having a diverse set of projects to observe,
our selected case study is OpenStack, the cloud computing infrastructure. 
We are performing a large empirical study on its more than 200 active projects.
\\
Actually considered the future of cloud, 
our choice is OpenStack because it is a large project that has adopted code
reviews on a large scale. Using modern software systems, it has a reasonable traceability 
of information of its software development process.
\\
OpenStack uses Launchpad, a bugtracking system for tracking
the issue reports, and Gerrit, a lightweight code review tool. 
Additionally, being an open source project, it is backed by a global collaboration
of developers.
\\
Besides OpenStack, code review is applied on a varied array of software projects. 
These projects have highly different settings, incentive structures and schemes, 
time pressures, and cultures.
\\
In an effort to characterize and understand these differences, previous studies~(\cite{bib01,bib02,bib03}) 
have examined open and non open source projects like Android OS, Chromium OS, Bing, Office, MS SQL, 
and projects internal to AMD. 
\\
They analysed how time to carry out the review process varies in these different projects. 
Some conclusions found that the practice of code review is slowing down the code integration activity. 
They found that in some cases not only the bug triaging is taking too long, but also that the median time 
for code review is days long, if not weeks.
\\
In line with the previous studies, we compared our findings on CRT with the results obtained for this 
metric from these studies.
Looking at the results, our main assumption is that if some of the code review metrics in different projects have become 
similar as the projects have normally advanced, then such metrics may be indicative of practices 
that may represent commonly successful and efficient methods of review.
\\
As such, these can be prescriptive to other projects taking into consideration to add code 
review to their development process.
\\
\\
In the remainder of this paper, we first describe some previous work on the topic (section II) and 
the background of the case study (section III). Next we detail the methodology (section IV). After  
describing the analysis (Section V) and displaying results (section VI), we discuss the conclusion and future work 
(section VII).
%
%


\section{Previous Work}
% no \IEEEPARstart

The purpose of code review is to find and fix the source code defects that can cause vulnerabilities 
and thus give rise to an exploit. Code review is characterized as \emph{``a systematic approach to examine 
a product in detail, using a predefined sequence of steps to determine if the product is fit for its
intended use''}~\cite{bib04}.
\\
The formal review or inspection according to Fagan's approach~\cite{bib05}, introduced 40 years ago, 
required the conduction of an inspection meeting for in fact finding defects.
There have been different ways of performing defect detection during these 40 years, from meeting based 
to meetingless based inspections. 
\\
Several experiments proved that no significant differences were found in the outcome between 
meeting and meetingless based inspections of the review process~(\cite{bib06,bib07}).
Another experiment~\cite{bib08} proved that time for conducting the review process significantly 
improved with meetingless based approaches.
\\
As a result, a considerate number of mechanisms and techniques were developed to code review, which 
results nowadays in the modern and ligh-weight code review process.
From the tool based static analysis~(\cite{bib09,bib010,bib011}) which examines the code in the absence
of input data and without running the code, to the widely
adopted modern code review~(\cite{bib012,bib013,bib014}) which, aligning with
the distributed nature of many projects, is asynchronous and
frequently supporting geographically distributed reviewers.
\\
Today, as a consequence of the many benefits, code review is a standard process of the modern software 
engineering workflow.
It is generally accepted that the performance of code review is associated with the effort spent to carry 
it out and is influenced by a range of factors, some of which are external to the technical aspects of 
the process~\cite{bib015}.
\\
In the practical guide to code review, Wiegers denotes that \emph{in formal inspection 
process the similarities outweigh their differences}~\cite{bib016}.
Furthermore, in their study on convergent practices to modern code review, Rigby and Bird 
compared parameters of review, such as review interval and the number of comments in 
review discussions. They also found that \emph{while there were some minor divergences in contemporary
practice, the findings converged}~\cite{bib01}, bringing them to conclude that even in modern code review 
the \emph{similarities outweigh their differences}. 


\section{Case Study Background}

This section provides background information about OpenStack and the tools used for 
obtaining data from its repositories.
\\
OpenStack is an open source set of software tools for building and
managing cloud computing platforms. Its community has
collaboratively identified 9 key components as the core part of OpenStack. 
Additional components are add to OpenStack from partners with the purpose 
of helping them to meet their needs. This is why, actually, in
OpenStack there are more than 200 active projects.
\\
OpenStack uses Launchpad as its issue tracking system. It is a repository
that enables users and developers to report \emph{defects} and \emph{feature} requests.
\\
Once a \emph{defect} (or issue) is reported, developers move to triage it. If it is deemed important, 
the issue is assigned to team members. Launchpad records the discussions that happen 
with any interested team member and track the history of all work on the issue. 
During these issue discussions, team members can ask questions, share their opinions 
and help other team members.
\\
\emph{Features} are also registered in Launchpad. 
First features are splitted in a list of working items called \emph{specifications} and then 
implemented.
But Launchpad is missing some functions that would make iterating on specifications design and approval usable, 
like ability to discuss or iterate on several revisions of a specification, or record multiple approvals. 
Thus some projects decided to experiment with using a specific git repository (\emph{*-specs}) to propose, discuss, 
iterate and track approvals on specifications using Gerrit instead of Launchpad. 
Those projects still ultimately use Launchpad once the specification is approved, 
to track the implementation status of the approved feature. 
\\
OpenStack uses a dedicated reviewing environment, Gerrit, to implement features, review patches
and bug fixes. It supports lightweight processes for reviewing code changes,
i.e., to decide whether a developer’s change is safe to integrate into the official
Version Control System (VCS). During this process, assigned reviewers make
comments on a code change or ask questions that can lead to a discussion of
the change and/or different revisions of the code change, before a final decision
is made about the code change. If accepted, the most recent revision of the
code change can enter the VCS, otherwise the change is abandoned and the
developer will move on to something else \cite{bib017}.
\\
To obtain the issue reports and code review data of these ecosystems, we
used the MetricsGrimoire toolset \cite{bib018} provided by Bitergia to mine the repositories of OpenStack, 
then store the corresponding data into a relational database.

%\subsubsection{Methodology}
\section{Methodology}
%data extraction - processing - analysis ...
%\\
To investigate the metrics that we proposed, we collected all the data about 
the code review process, from the intention, which corresponds to the moment when 
a report describing a possible bug is opened, 
up to the moment the fix, for that bug, runs in the code base of the project.
\\
In order to achieve this, we first extracted the code review process 
information from Launchpad and Gerrit.
\\
We then preprocessed the data, identified the attributes for
measuring the metrics, and finally performed our analysis.
The operations of data extraction and processing were automatic. 
\\
In order to ensure the quality 
of the data collection,
after every applied heuristic, we manually analysed the
selection picking up a number of random elements.
Finally, we related the results of our analysis with that of the previous
studies and draw our results.

\subsection{Data Extraction and Preprocessing}

In our previous work~(\cite{bib019,bib020}), we have described the extraction process and the 
resulting dataset for both Launchpad\footnote{activity.openstack.org/dash/browser/data/db/tickets.mysql.7z} 
and Gerrit\footnote{activity.openstack.org/dash/browser/data/db/reviews.mysql.7z} repositories, 
using the Metrics Grimoire~\cite{bib018} toolset. 
The data is also available for other researchers to use. 
%Remember to reference the paper from SOMET
%This first of these works did not involve the 
%analysis of all the time spend from intention until the code runs into the codebase, while 
%the second did not involve analysis of the code review process data. 
\\
In the remainder of this section, we
discuss what implies the bug triaging and review activity for the project, and briefly
describe how we extracted all the information necessary to analyse each metric.
\\
\subsubsection{Bug Triaging}

This metric is a measure of the time that developers need to
identify if an issue, that has been reported, 
is a bug. How do developers 
identify which ticket \footnote{in Launchpad, the report of an issue is called a ticket.} is describing a bug?
This is not straightforward in OpenStack. Analysing
the Launchpad work flow, we came across a pattern in the
evolution of a ticket's states. While going through the bug triaging activity, 
developers use different states for a ticket, 
in different stages, which allows the developers to indicate that a given ticket 
is describing a bug.
When a ticket, stating a possible bug, is opened in
Launchpad, its status is set to \emph{New}.
If the problem described in the ticket is reproduced,
the bug is confirmed as genuine and the ticket status
changes from \emph{New} to \emph{Confirmed}.
Only when a bug is confirmed, the status then changes
from \emph{Confirmed} to \emph{In Progress} the moment when an
issue is opened for review in Gerrit.
\\
Thus, we analysed the Launchpad repository searching for
tickets that match with this pattern. These are the tickets
that have been classified as bug reports. 
\\Once identified, we extracted them in a new repository for further inspection.
Our results showed that, begining from 2010 up to January
2017, in Launchpad, 83.254 tickets out of 123.997 have been
classified as bugs. Hence approximately 67\% of the total
tickets in Launchpad have been reproduced as genuine
bugs, and an issue for fixing them has been respectively opened in Gerrit.
\\
At this point we are able to quantify the time that developers spend during the 
bug triaging activity as the distance in
time between the moment when a ticket is first opened in
Launchpad up to the moment it is confirmed as a genuine
bug.
\\
\subsubsection{Review Time}

The next step is to link the tickets (bugs) that we have already
extracted from the issue tracking system with their 
respective review the Gerrit code review system. Traceability of
this linkage is not a trivial task in OpenStack.
\\
To detect the links between ticket and reviews, we referred to 
the information provided in the comments of the tickets. 
When a proposal for a fix, or a merge for a fix, is received, 
it's information is reported in the comments of the respective issue.
%add an example of comment
\\
But, the first problem that arises from the comments is that 
in some cases they are a summary of the commit history. 
In those cases we find more than a matching review 
with the issue we are analysing. But not all these commits are merged in
the master branch of the project that originated the ticket. Hence we 
obtain the incorrect matching bug-review.
\\
However, manually analysing a number of random tickets, we noted that 
the comments reporting a merge do have a structured format. 
In this structure, the information related to the review
is stated at the head of the comment. More specifically, the 6th 
attribute of this structure provides the branch where the current fix 
was merged.
\\
Thus the first step, towards the match bug-review, is trunking the comments. 
We extracted the comment's head, exactly the first 6 lines from every one of them. 
By doing this, we are able to match each bug with the review process that has provided 
a fix for it into the master branch of the project.
\\
At this point we are able to quantify the time to review in
OpenStack as the distance in time from when an issue for fixing 
the bug is started 
%in the first patch is uploaded 
in Gerrit up to when a fix is merged to the code base (master branch of the project).

\section{Analysis}

In this section we display the results that were obtained from the analysis 
of OpenStack project for both metrics: bug triaging time and review time. 
Our purpose usefulness was discussed in section I.
\\ \\
% MAYBE move this to purpose section
The researchers community is continuously gaining interest in those two metrics, 
analysing them in different experiments (for example~\cite{bib01,bib02,bib03,bib019,bib021} etc). 
But while there are studies that have quantified time to review~\cite{bib01,bib02}, at the best 
of our knowledge still no study has done the same for bug triaging.
\\ \\
Despite this, more and more attention is directed at understanding the impact that bug triaging 
has on the overall review process and to what factors influence this activity.
\\ %
% MAYBE move this to conclusion section
On one hand the industries put a lot of effort in individuating and fixing 
defects injected by development. But on the other hand they need the review to be short, 
because a long lasting process usually is not that effective. 
\\ \\
A long review may generate process stalls and affects whoever might be waiting to take 
a dependency on the new code.
Additionally, the longer the review time is, the more complicated is for the author to move back to 
the change and integrate the feedback of the reviewers without potentially introducing new defects. 
\\ \\
This is why, in this context, we underline the importance of maintaining the process time as 
controlled as possible. And the two most important elements influencing this time are precisely 
bug triaging and review time. By supervising these metrics the review process time can be as 
short or long as according to the case. 
\\ %
\\
In order to show our assumption we first analysed the ICR process, 
then extracted the two metrics of BTT and CRT, with the purpose to better 
understand the inlfuence that these metrics have on ICR.
\\
\\
The data we are analysing from OpenStack, comprises all the historical span from 2010 to January 2017. 
As we mentioned in section IV, in this overall history, 83.254 tickets were classified as bugs. 
For our analysis, we are considering 
only the fixed bugs, which change has been merged into the code base, thus excluding the reviews still in 
progress, abandoned and duplicated issues. 
\\
\\Therefore, the total number of tickets we have analysed is 59.209, which is approximately 71\% of the total 
number of identified bugs. 

\section{Results}
We computed the three measures from Intention to Code Review (ICR), Bug Triaging Time (BTT) 
and Code Review Time (CRT) as the following: 
\\
\\
\emph{ICR} - the time from when the ticket is opened in the issue tracking system, to 
when the fix for the bug is merged into the code base;
\\
\emph{BTT} - the time from the moment when the ticket is opened in
Launchpad up to the moment it is \emph{Confirmed} as a genuine bug.
\\
\emph{CRT} - the time from when the first patch is uploaded in Gerrit up to when the 
change is merged to the code base.
\\
\\
Afterwards, we calculated the median effect size across all Open Stack projects history 
in order to globally rank the metrics from most extreme effect size.
\\
Additionally, we calculate and expose the resuts for the ICR, BTT and CRT per quantiles.
\\
\\
We found that the median time for: 

a. bug triaging in OpenStack (Launchpad) is 0.9 days,

b. code review in OpenStack (Gerrit) is 2.1 days,

c. intention to code review in OpenStack is 14.1 days. 
\\
\\
The results of the quantiles for BTT, TTR and ICR are shown in the table \ref{fig:1}, then additionally 
the same results are shown in graphical form \ref{fig:2} to visually let us understand better what portion of ICR occupy 
BTT and TTR. The x-axis represents the percentage of tickets, while the y-axis represents the 
number of days. The results in both Fig. \ref{fig:1} and Fig. \ref{fig:2} are measured in days:

\begin{figure}[H]
\centering

\includegraphics[width=0.45\textwidth,natwidth=397,natheight=96]{wetsom1.png}

\caption{The quantiles for BTT, TTR and ICR measured in days.}
\label{fig:1}
\end{figure}

\begin{figure}[H]
\centering

\includegraphics[width=0.45\textwidth,natwidth=462,natheight=282]{wetsom2.png}

\caption{Chart of the quantiles for BTT, TTR and ICR measured in days.}
\label{fig:2}
\end{figure}

\section{Conclusions and Future Work}

%In the practical guide to code review, Wiegers denotes that \emph{in formal inspection 
%process the similarities outweigh their differences}~\cite{bib21}.
%Furthermore, in their study on convergent practices to modern code review, Rigby and Bird 
%compared parameters of review, such as review interval and the number of comments in 
%review discussions. They also found that \emph{while there were some minor divergences in contemporary
%practice, the findings converged}~\cite{bib13}, bringing them to conclude that even in modern code review 
%the \emph{similarities outweigh their differences}.  

As previously mentioned, related the results obtained from our analysis with 
that of the previous studies~\cite{bib01,bib022}, 
in an effort to characterize and understand the differences. These
studies have analysed the review process for some of
the AMD, Microsoft, and Google-led projects. The results from these studies 
along with our analysis of OpenStack are shown in the table below (Fig. \ref{fig:3}):

\begin{figure}[H]
\centering

\includegraphics[width=0.25\textwidth,natwidth=205,natheight=235]{wetsom3.png}

\caption{Time To Review (TTR) in  different projects.}
\label{fig:3}
\end{figure}


Additionally in the table below, Fig. \ref{fig:4} we add more context to the results by 
displaying characteristics of the projects analysed: the time period examined in years 
and the number of reviews.

\begin{figure}[H]
\centering

\includegraphics[width=0.35\textwidth,natwidth=260,natheight=235]{wetsom4.png}

\caption{Time To Review (TTR) in  different projects.}
\label{fig:4}
\end{figure}

The TTR metric has been studied in a number of experiments over the
last 40 years~(\cite{bib05,bib03,bib023}). 
Our contribution here is to add the large cloud computing project, such as OpenStack, 
to the existing diverse set of projects analysed on this parameter.
\\
The results displayed both in Fig. \ref{fig:3} and \ref{fig:4}, show that 
despite differences among projects, the parameter of TTR have similar values.
%, which we think indicate general and common principles of code review practices.
Our main assumption is that if some of the code review metrics  
in different projects have become similar as the projects have  
normally advanced, then such metrics may be indicative of  
practices that may represent commonly successful and efficient  
methods of review.
\\
The only measure we have, at the best of our knowledge, for the BTT metric are some indications 
provided by \cite{bib024} and \cite{bib025}, two experiments respectively conducted with the support 
data from Microsoft and Bugzilla. They found that 
\emph{bug triage takes up a large amount of developers resources and it goes from about 24 hours to 
many lasting days, if not weeks}. Our contribution here is to bring some numbers on how long this process 
takes in a large cloud computing project, such as OpenStack.

The final purpose of this paper is to highlight, from the ICR process, the two metrics of BTT and TTR, 
and recognize them as key metrics to 
supervise during the review process. You can't control what you can't measure.
And by controlling these metrics, not only the effectiveness of code review can be quantified and the progress of 
a development team can be measured, but also policies and guidelines, which can improve the development process, 
may arise.
As such, these metrics can be prescriptive to other projects taking into consideration to add code review to 
their development process. 
\\Additionally, in a continuous integration and deployment environment, such as OpenStack, 
the role that the two activities of bug triaging and code review have on the overall process of ICR (Fig. \ref{fig:1} 
and \ref{fig:2}) becomes of vital importance. Not only the quality of the code under development must be assured, 
but also the time waiting to take possesion of the new code must be relatively as short as possible. 
Thus controlling ICR, and the parameters that affect it, including here BTT and TTR, becomes crucial to the success of 
a project itself.
\\
\\
Taking into consideration the above discussion, our future work is to continue analysing 
(along with ICR, BTT and TTR) more phases 
from the software development process. We highlight the fact that also the feature implementation follows the 
same workflow in Gerrit. 
\\Thus, our work will be aimed at characterizing various phases highlighting those parameters that 
influence the performance of the process, and define what factors influence the paramenters itself.
The purpose of our work is raising specific metrics as key performance indicators (KPIs). 
They will be KPIs that when controled are able to improve the software development process 
either by discovering software development policies or guidelines, and by setting configurations of 
a development team. 
 



%\section{Type style and Fonts}
%Wherever Times is specified, Times Roman or Times New Roman may be used. If neither is available 
%on your system, please use the font closest in appearance to Times. Avoid using bit-mapped fonts 
%if possible. True-Type 1 or Open Type fonts are preferred. Please embed symbol fonts, as well, 
%for math, etc.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.


% conference papers do not normally have an appendix


% use section* for acknowledgement
\section*{Acknowledgment}


The authors would like to thank SENECA, EID project under
Marie-Skodowska Curie Actions, for funding one of the authors, and 
Bitergia company for the Metrics Grimoire toolset, which realised the 
extraction of the dataset used in this paper.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}


\bibitem{bib01}
Peter~C.~Rigby and Christian~Bird. 2013. \emph{Convergent
contemporary software peer review practices.} In
Proceedings of the 2013 9th Joint Meeting on
Foundations of Software Engineering
(ESEC/FSE2013). ACM, New York, NY, USA, 202-212.

\bibitem{bib02}
Amiangshu~Bosu and Jeffrey~Carver. 2013. \emph{Impact of
Peer Code Review on Peer Impression Formation: A
Survey.} Proceedings of the 7th ACM/IEEE
International Symposium on Empirical Software
Engineering and Measurement (ESEM), 2013.
Baltimore, MD, USA, 133-142.

\bibitem{bib03}
A.~Porter, H.~Siy, A.~Mockus, and L.~Votta.
\emph{Understanding the sources of variation in software
inspections.} ACM Transactions Software Engineering
Methodology, 7(1):4 1-79, 1.


\bibitem{bib04}
D.~L.~Parnas and M.~Lawford. \emph{Inspection’s role in
software quality assurance.} In Software, IEEE, vol.
20, 2003.

\bibitem{bib05}
M.~E.~Fagan. \emph{Design and Code inspections to reduce
errors in program development.} In IBM Systems
Journal 15 pp. 182-211, 1976.

\bibitem{bib06}
P.~M.~Johnson, and D.~Tjahjono. \emph{Does Every
Inspection Really Need a Meeting?} In Empirical
Software Engineering, vol. 3, no. 1, pp. 9-35, 1998.

\bibitem{bib07}
P.~McCarthy, A.~Porter, H.~Siy et al. \emph{An experiment
to assess cost-benefits of inspection meetings and
their alternatives: a pilot study.} In Proceedings of the
3rd International Symposium on Software Metrics:
From Measurement to Empirical Results, 1996.

\bibitem{bib08}
A.~Porter, H.~Siy, C.~A. Toman et al. \emph{An experiment
to assess the cost-benefits of code inspections in large
scale software development.} In SIGSOFT Softw. Eng.
Notes, vol. 20, no. 4, pp. 92-103, 1995.

\bibitem{bib09}
W.~R.~Bush, J.~D.~Pincus, D.~J.~Sielaff. \emph{A static
analyzer for finding dynamic programming errors.}
Softw. Pract. Exper. , vol. 30, no. 7, pp. 775-802,
2000.

\bibitem{bib010}
Hallem, D.~Park, and D.~Engler. \emph{Uprooting software
defects at the source.} Queue, vol. 1, no. 8, pp. 64-71,
2003.

\bibitem{bib011}
B.~Chess and J.~West. \emph{Secure Programming with
Static Analysis.} 1st ed. Addison-Wesley Professional,
Jul. 2007.

\bibitem{bib012}
N.~Kennedy. \emph{How google does web-based code reviews
with mondrian.} http://www.test.org/doe/, Dec. 2006.

\bibitem{bib013}
A.~Tsotsis. \emph{Meet phabricator, the witty code review
tool built inside facebook.}
http://techcrunch.com/2011/08/07/oh-what-noble-
scribe-hath- penned-these-words/, Aug.
2006.

\bibitem{bib014}
Gerrit code review - https://www.gerritcodereview.com/.

\bibitem{bib015}
Baysal,~O., Kononenko,~O., Holmes,~R., Godfrey,~M.~W. 
(2015). \emph{Investigating technical and non-technical 
factors influencing modern code review.} Empirical
Software Engineering, 1-28.

\bibitem{bib016}
K.~E.~Wiegers. \emph{Peer Reviews in Software: A Practical 
Guide. Addison-Wesley Information Technology Series.} 
Addison-Wesley, 2001.

\bibitem{bib017}
http://docs.openstack.org/infra/manual/developers.html

\bibitem{bib018}
J.~M.~Gonzalez-Barahona, G.~Robles, and D.~Izquierdo-Cortazar. 
\emph{The metricsgrimoire database
collection.} In 12th Working Conference on Mining
Software Repositories (MSR), pages 478-481, May
2015.

\bibitem{bib019}
Dorealda~Dalipaj. \emph{A quantitative analysis of
performance in the key parameter in code review -
Individuation of defects.} Proceedings of the Doctoral Consortium at the 12th 
International Conference on Open Source Systems (OSS) 2016, Gothenburg, Sweden, 
11-24.

\bibitem{bib020}
Dorealda~Dalipaj, Jesus M. Gonzalez-Barahona, Daniel Izquierdo-Cortazar.
\emph{Software engineering artifact in software development process – 
Linkage between issues and code review processes.}
Proceedings of the 15th International Conference on New Trends 
in Intelligent Software Methodology Tools and Techniques (SoMeT) 2016, 
Larnaca, Cyprus, 115-122.

\bibitem{bib021}
P.~C.~Rigby, D.~M.~German, and M.A.~Storey. \emph{Open
source software peer review practices: A case study of
the apache server.} In ICSE: Proceedings of the 30th
international conference on Software Engineering,
pages 541-550, 2008.

\bibitem{bib022}
P.~C.~Rigby. Understanding Open Source Software Peer 
Review: Review Processes, Parameters and Statistical 
Models, and Underlying Behaviours and Mechanisms. 
http://hdl.handle.net/1828/3258. University of Victoria, 
Canada, Dissertation, 2011.

\bibitem{bib023}
P.~C.~Rigby, D.~M.~German, and  M.~A.~Storey. \emph{Open
source software peer review practices: A case study of the apache server.} 
In ICSE: Proceedings of the 30th international conference on Software Engineering, 
pag. 541–550, 2008.

\bibitem{bib024}
Czerwonka~Jacek, Michaela~Greiler and Jack~Tilford.
\emph{Code Reviews Do Not Find Bugs. How the Current
Code Review Best Practice Slows Us Down.}
Proceedings of the 2015 International Conference on
Software Engineering. IEEE Publisher, 2015.  

\bibitem{bib025}
Tao~Zhang, Byungjeong~Lee. \emph{A Bug Rule Based Technique with Feedback for 
Classifying Bug Reports.} Computer and Information Technology (CIT), 2011, 
IEEE 11th International Conference, pp 336-343.

\end{thebibliography}




% that's all folks
\end{document}


\grid
